Net Architecture:
Resnet18Skip(
  (res_conv0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (res_conv1): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (res_conv2): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (res_conv3): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (res_conv4): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (skip_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (skip_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (skip_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (seg_conv): Sequential(
    (0): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)
    (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (2): ReLU()
    (3): Conv2d(64, 5, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 4
lr: 0.001
epochs: 30
batch_size: 50
weight_decay: 0.0001
scheduler_step: 10
scheduler_gamma: 0.1
output_folder: pretrained_backbone
load_best: False
gpu_ids: 0
log_freq: 20
dataset_dir: /home/zheyu/catkin_ws/src/rvss_gazebo/dataset/random_texture
target_bucket: 
===========================================================
============= Epoch 0 | 2021-01-17 17:42:05 ===============
=> Current Lr: 0.001
[0/216]: 1.5284
[20/216]: 0.0595
[40/216]: 0.0428
[60/216]: 0.0323
[80/216]: 0.0217
[100/216]: 0.0192
[120/216]: 0.0109
[140/216]: 0.0124
[160/216]: 0.0085
[180/216]: 0.0160
[200/216]: 0.0124
=> Training Loss: 0.0348, Evaluation Loss 0.0119

============= Epoch 1 | 2021-01-17 17:42:46 ===============
=> Current Lr: 0.001
[0/216]: 0.0113
[20/216]: 0.0089
[40/216]: 0.0064
[60/216]: 0.0064
[80/216]: 0.0056
[100/216]: 0.0062
[120/216]: 0.0087
[140/216]: 0.0075
[160/216]: 0.0103
[180/216]: 0.0181
[200/216]: 0.0081
=> Training Loss: 0.0086, Evaluation Loss 0.0090

============= Epoch 2 | 2021-01-17 17:43:28 ===============
=> Current Lr: 0.001
[0/216]: 0.0082
[20/216]: 0.0075
[40/216]: 0.0083
[60/216]: 0.0050
[80/216]: 0.0057
[100/216]: 0.0083
[120/216]: 0.0074
[140/216]: 0.0057
[160/216]: 0.0056
[180/216]: 0.0089
[200/216]: 0.0069
=> Training Loss: 0.0067, Evaluation Loss 0.0289

============= Epoch 3 | 2021-01-17 17:44:10 ===============
=> Current Lr: 0.001
[0/216]: 0.0086
[20/216]: 0.0116
[40/216]: 0.0061
[60/216]: 0.0068
[80/216]: 0.0051
[100/216]: 0.0070
[120/216]: 0.0061
[140/216]: 0.0076
[160/216]: 0.0056
[180/216]: 0.0052
[200/216]: 0.0060
=> Training Loss: 0.0078, Evaluation Loss 0.0118

============= Epoch 4 | 2021-01-17 17:44:53 ===============
=> Current Lr: 0.001
[0/216]: 0.0091
[20/216]: 0.0056
[40/216]: 0.0049
[60/216]: 0.0052
[80/216]: 0.0053
[100/216]: 0.0047
[120/216]: 0.0055
[140/216]: 0.0070
[160/216]: 0.0067
[180/216]: 0.0069
[200/216]: 0.0044
=> Training Loss: 0.0063, Evaluation Loss 0.0061

============= Epoch 5 | 2021-01-17 17:45:36 ===============
=> Current Lr: 0.001
[0/216]: 0.0053
[20/216]: 0.0090
[40/216]: 0.0051
[60/216]: 0.0046
[80/216]: 0.0040
[100/216]: 0.0041
[120/216]: 0.0046
[140/216]: 0.0052
[160/216]: 0.0043
[180/216]: 0.0050
[200/216]: 0.0046
=> Training Loss: 0.0058, Evaluation Loss 0.0136

============= Epoch 6 | 2021-01-17 17:46:18 ===============
=> Current Lr: 0.001
[0/216]: 0.0056
[20/216]: 0.0050
[40/216]: 0.0043
[60/216]: 0.0053
[80/216]: 0.0046
[100/216]: 0.0047
[120/216]: 0.0052
[140/216]: 0.0055
[160/216]: 0.0047
[180/216]: 0.0046
[200/216]: 0.0048
=> Training Loss: 0.0057, Evaluation Loss 0.0062

============= Epoch 7 | 2021-01-17 17:47:00 ===============
=> Current Lr: 0.001
[0/216]: 0.0053
[20/216]: 0.0060
[40/216]: 0.0042
[60/216]: 0.0051
[80/216]: 0.0061
[100/216]: 0.0071
[120/216]: 0.0075
[140/216]: 0.0054
[160/216]: 0.0047
[180/216]: 0.0062
[200/216]: 0.0049
=> Training Loss: 0.0057, Evaluation Loss 0.0062

============= Epoch 8 | 2021-01-17 17:47:41 ===============
=> Current Lr: 0.001
[0/216]: 0.0047
[20/216]: 0.0037
[40/216]: 0.0032
[60/216]: 0.0045
[80/216]: 0.0044
[100/216]: 0.0043
[120/216]: 0.0041
[140/216]: 0.0037
[160/216]: 0.0040
[180/216]: 0.0055
[200/216]: 0.0044
=> Training Loss: 0.0049, Evaluation Loss 0.0051

============= Epoch 9 | 2021-01-17 17:48:24 ===============
=> Current Lr: 0.001
[0/216]: 0.0043
[20/216]: 0.0051
[40/216]: 0.0053
[60/216]: 0.0069
[80/216]: 0.0033
[100/216]: 0.0041
[120/216]: 0.0110
[140/216]: 0.0047
[160/216]: 0.0042
[180/216]: 0.0043
[200/216]: 0.0054
=> Training Loss: 0.0054, Evaluation Loss 0.0136

============= Epoch 10 | 2021-01-17 17:49:06 ==============
=> Current Lr: 0.0001
[0/216]: 0.0045
[20/216]: 0.0041
[40/216]: 0.0040
[60/216]: 0.0043
[80/216]: 0.0032
[100/216]: 0.0029
[120/216]: 0.0039
[140/216]: 0.0036
[160/216]: 0.0030
[180/216]: 0.0029
[200/216]: 0.0030
=> Training Loss: 0.0037, Evaluation Loss 0.0031

============= Epoch 11 | 2021-01-17 17:49:48 ==============
=> Current Lr: 0.0001
[0/216]: 0.0029
[20/216]: 0.0028
[40/216]: 0.0034
[60/216]: 0.0024
[80/216]: 0.0026
[100/216]: 0.0037
[120/216]: 0.0025
[140/216]: 0.0024
[160/216]: 0.0025
[180/216]: 0.0028
[200/216]: 0.0034
=> Training Loss: 0.0029, Evaluation Loss 0.0030

============= Epoch 12 | 2021-01-17 17:50:31 ==============
=> Current Lr: 0.0001
[0/216]: 0.0032
[20/216]: 0.0022
[40/216]: 0.0027
[60/216]: 0.0026
[80/216]: 0.0027
[100/216]: 0.0032
[120/216]: 0.0041
[140/216]: 0.0023
[160/216]: 0.0029
[180/216]: 0.0022
[200/216]: 0.0024
=> Training Loss: 0.0027, Evaluation Loss 0.0027

============= Epoch 13 | 2021-01-17 17:51:14 ==============
=> Current Lr: 0.0001
[0/216]: 0.0023
[20/216]: 0.0024
[40/216]: 0.0020
[60/216]: 0.0023
[80/216]: 0.0023
[100/216]: 0.0040
[120/216]: 0.0029
[140/216]: 0.0021
[160/216]: 0.0025
[180/216]: 0.0022
[200/216]: 0.0024
=> Training Loss: 0.0025, Evaluation Loss 0.0026

============= Epoch 14 | 2021-01-17 17:51:57 ==============
=> Current Lr: 0.0001
[0/216]: 0.0019
[20/216]: 0.0028
[40/216]: 0.0033
[60/216]: 0.0024
[80/216]: 0.0025
[100/216]: 0.0020
[120/216]: 0.0019
[140/216]: 0.0021
[160/216]: 0.0021
[180/216]: 0.0022
[200/216]: 0.0024
=> Training Loss: 0.0024, Evaluation Loss 0.0026

============= Epoch 15 | 2021-01-17 17:52:38 ==============
=> Current Lr: 0.0001
[0/216]: 0.0026
[20/216]: 0.0019
[40/216]: 0.0027
[60/216]: 0.0022
[80/216]: 0.0024
[100/216]: 0.0026
[120/216]: 0.0023
[140/216]: 0.0022
[160/216]: 0.0019
[180/216]: 0.0026
[200/216]: 0.0018
=> Training Loss: 0.0024, Evaluation Loss 0.0024

============= Epoch 16 | 2021-01-17 17:53:20 ==============
=> Current Lr: 0.0001
[0/216]: 0.0020
[20/216]: 0.0024
[40/216]: 0.0018
[60/216]: 0.0023
[80/216]: 0.0020
[100/216]: 0.0022
[120/216]: 0.0026
[140/216]: 0.0029
[160/216]: 0.0024
[180/216]: 0.0019
[200/216]: 0.0028
=> Training Loss: 0.0023, Evaluation Loss 0.0027

============= Epoch 17 | 2021-01-17 17:54:03 ==============
=> Current Lr: 0.0001
[0/216]: 0.0018
[20/216]: 0.0030
[40/216]: 0.0021
[60/216]: 0.0018
[80/216]: 0.0021
[100/216]: 0.0017
[120/216]: 0.0018
[140/216]: 0.0019
[160/216]: 0.0018
[180/216]: 0.0024
[200/216]: 0.0024
=> Training Loss: 0.0021, Evaluation Loss 0.0026

============= Epoch 18 | 2021-01-17 17:54:45 ==============
=> Current Lr: 0.0001
[0/216]: 0.0021
[20/216]: 0.0021
[40/216]: 0.0023
[60/216]: 0.0022
[80/216]: 0.0020
[100/216]: 0.0020
[120/216]: 0.0018
[140/216]: 0.0027
[160/216]: 0.0022
[180/216]: 0.0017
[200/216]: 0.0020
=> Training Loss: 0.0022, Evaluation Loss 0.0023

============= Epoch 19 | 2021-01-17 17:55:27 ==============
=> Current Lr: 0.0001
[0/216]: 0.0018
[20/216]: 0.0019
[40/216]: 0.0029
[60/216]: 0.0017
[80/216]: 0.0028
[100/216]: 0.0020
[120/216]: 0.0019
[140/216]: 0.0021
[160/216]: 0.0021
[180/216]: 0.0020
[200/216]: 0.0022
=> Training Loss: 0.0021, Evaluation Loss 0.0024

============= Epoch 20 | 2021-01-17 17:56:09 ==============
=> Current Lr: 1e-05
[0/216]: 0.0019
[20/216]: 0.0022
[40/216]: 0.0017
[60/216]: 0.0016
[80/216]: 0.0020
[100/216]: 0.0021
[120/216]: 0.0022
[140/216]: 0.0019
[160/216]: 0.0014
[180/216]: 0.0017
[200/216]: 0.0018
=> Training Loss: 0.0020, Evaluation Loss 0.0021

============= Epoch 21 | 2021-01-17 17:56:51 ==============
=> Current Lr: 1e-05
[0/216]: 0.0022
[20/216]: 0.0014
[40/216]: 0.0021
[60/216]: 0.0016
[80/216]: 0.0019
[100/216]: 0.0017
[120/216]: 0.0017
[140/216]: 0.0015
[160/216]: 0.0016
[180/216]: 0.0018
[200/216]: 0.0018
=> Training Loss: 0.0019, Evaluation Loss 0.0022

============= Epoch 22 | 2021-01-17 17:57:33 ==============
=> Current Lr: 1e-05
[0/216]: 0.0018
[20/216]: 0.0020
[40/216]: 0.0021
[60/216]: 0.0017
[80/216]: 0.0017
[100/216]: 0.0022
[120/216]: 0.0027
[140/216]: 0.0017
[160/216]: 0.0021
[180/216]: 0.0017
[200/216]: 0.0018
=> Training Loss: 0.0019, Evaluation Loss 0.0023

============= Epoch 23 | 2021-01-17 17:58:15 ==============
=> Current Lr: 1e-05
[0/216]: 0.0016
[20/216]: 0.0016
[40/216]: 0.0023
[60/216]: 0.0022
[80/216]: 0.0019
[100/216]: 0.0024
[120/216]: 0.0018
[140/216]: 0.0019
[160/216]: 0.0016
[180/216]: 0.0020
[200/216]: 0.0018
=> Training Loss: 0.0018, Evaluation Loss 0.0021

============= Epoch 24 | 2021-01-17 17:58:57 ==============
=> Current Lr: 1e-05
[0/216]: 0.0020
[20/216]: 0.0020
[40/216]: 0.0025
[60/216]: 0.0024
[80/216]: 0.0018
[100/216]: 0.0017
[120/216]: 0.0018
[140/216]: 0.0019
[160/216]: 0.0017
[180/216]: 0.0017
[200/216]: 0.0033
=> Training Loss: 0.0018, Evaluation Loss 0.0021

============= Epoch 25 | 2021-01-17 17:59:39 ==============
=> Current Lr: 1e-05
[0/216]: 0.0019
[20/216]: 0.0018
[40/216]: 0.0018
[60/216]: 0.0015
[80/216]: 0.0020
[100/216]: 0.0018
[120/216]: 0.0017
[140/216]: 0.0020
[160/216]: 0.0017
[180/216]: 0.0016
[200/216]: 0.0017
=> Training Loss: 0.0018, Evaluation Loss 0.0021

============= Epoch 26 | 2021-01-17 18:00:21 ==============
=> Current Lr: 1e-05
[0/216]: 0.0025
[20/216]: 0.0017
[40/216]: 0.0017
[60/216]: 0.0017
[80/216]: 0.0018
[100/216]: 0.0017
[120/216]: 0.0016
[140/216]: 0.0016
[160/216]: 0.0019
[180/216]: 0.0016
[200/216]: 0.0021
=> Training Loss: 0.0018, Evaluation Loss 0.0021

============= Epoch 27 | 2021-01-17 18:01:03 ==============
=> Current Lr: 1e-05
[0/216]: 0.0021
[20/216]: 0.0015
[40/216]: 0.0018
[60/216]: 0.0020
[80/216]: 0.0019
[100/216]: 0.0018
[120/216]: 0.0019
[140/216]: 0.0018
[160/216]: 0.0016
[180/216]: 0.0016
[200/216]: 0.0021
=> Training Loss: 0.0018, Evaluation Loss 0.0022

============= Epoch 28 | 2021-01-17 18:01:45 ==============
=> Current Lr: 1e-05
[0/216]: 0.0014
[20/216]: 0.0017
[40/216]: 0.0018
[60/216]: 0.0017
[80/216]: 0.0012
[100/216]: 0.0018
[120/216]: 0.0018
[140/216]: 0.0016
[160/216]: 0.0033
[180/216]: 0.0017
[200/216]: 0.0015
=> Training Loss: 0.0018, Evaluation Loss 0.0021

============= Epoch 29 | 2021-01-17 18:02:27 ==============
=> Current Lr: 1e-05
[0/216]: 0.0020
[20/216]: 0.0016
[40/216]: 0.0020
[60/216]: 0.0016
[80/216]: 0.0021
[100/216]: 0.0018
[120/216]: 0.0020
[140/216]: 0.0018
[160/216]: 0.0017
[180/216]: 0.0020
[200/216]: 0.0017
=> Training Loss: 0.0018, Evaluation Loss 0.0022
